# AI BASIC

## Parameter

### 파라미터(parameter)

- 파라미터는 AI 모델에서 학습을 통해 조정되는 **가중치(weight)**를 의미함.

- 이 값들은 모델이 입력을 보고 출력을 예측하도록 만드는 "지식 저장소" 역할을 함

- 예를들어

  ```
  y = W * x + b
  ```

  여기서 W(가중치)와 b(편향)이 파라미터. 이러한 값들을 조정하는게 학습(training)

- 학습 가능한 파라미터 vs. 학습된 파라미터

  - 학습 가능한 파라미터(trainable parameters)

    모델 구조 안에서 훈련 중 조정되는 변수들. 즉, "모델이 학습할 수 있는"부분

  - 학습된 파라미터

    실제 학습 과정을 거친 후 값이 최종적으로 정해진 파라미터

  - 30B?

    30B 모델이라 함은 30B개의 학습 가능한 파라미터를 가진 모델(모델 내부의 숫자 값이 300억 개 저장되어있다는 의미)이고, 학습이 끝난 후에는 그것들이 30B개의 학습된 파라미터로 남게 된다.

### 스케일링 법칙(Scaling Law)

- 스케일링 법칙은 모델의 크기(파라미터 수), 데이터 양, 계산양이 늘어날 때 성능이 어떻게 개선되는지를 수학적으로 설명한 경험 법칙

  이 법칙은 2020년 Open AI 논문

  > "Scaling Laws for Neural Language Models" (Kaplan et al. 2020) 에서 체계적으로 정리됨

- 결론적으로는 **모델이 커질수록 성능이 점점 좋아지지만, 그 향상 폭은 점점 작아진다**는 의미

- 실제 효과

  | 모델크기   | 파라미터 수   | 품질 향상 체감                        |
  | ---------- | ------------- | ------------------------------------- |
  | 1B → 10B   | 10배 증가     | 매우 큰 향상(문법, 문맥, 논리적 연결) |
  | 10B → 30B  | 3배 증가      | 눈에 띄는 향상(추론, 복합 질의 응답)  |
  | 30B → 70B  | 2배 이상 증가 | 미미한 향상                           |
  | 70B → 500B | 수십 배 증가  | 품질은 약간 상승, 비용은 폭발적 증가  |

  파라미터 수가 커질수록 품질은 좋아지지만 증가율은 점점 완만해진다.

- 따라서 단순히 "B 수가 크다" 보다 학습 데이터 품질, 파인튜닝 방식, 추론 최적화(RLHF) 등이 함께 중요함.